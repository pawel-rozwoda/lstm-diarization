{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import pickle5\n",
    "from aux import load_audio\n",
    "from config import DATA_PATH, GMM_FILE, OUT_TRAIN, CALLHOME_ENG_10_SEC\n",
    "from aux_evaluate import draw_speech_and_channel\n",
    "\n",
    "import numpy as np\n",
    "from kaldi_feats import logmel_feats\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_file = CALLHOME_ENG_10_SEC + os.listdir(CALLHOME_ENG_10_SEC)[19]\n",
    "audio_file = CALLHOME_ENG_10_SEC + '4289.wav'\n",
    "\n",
    "sample_rate, audio = load_audio(audio_file, 8000, mono=False)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "vad=None                                                                                                                                                    \n",
    "with open(GMM_FILE, 'rb') as fid:                                                                                                             \n",
    "    vad = pickle5.load(fid)                \n",
    "\n",
    "channel_0, channel_1 = audio[0], audio[1]\n",
    "speech_0 = vad.detect_speech(signal=channel_0, sampling_rate=8000, fit_to_audio=True)\n",
    "speech_1 = vad.detect_speech(signal=channel_1, sampling_rate=8000, fit_to_audio=True)\n",
    "\n",
    "overlapping_speech = np.logical_and(speech_0, speech_1)\n",
    "channel_0, channel_1 = channel_0/32768, channel_1/32768\n",
    "\n",
    "_, mono = load_audio(audio_file, 8000, mono=True)\n",
    "speech_mono = vad.detect_speech(signal=mono,sampling_rate=8000, fit_to_audio=True)\n",
    "\n",
    "draw_speech_and_channel(speech_mono, mono, 'mono', 'purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../train')\n",
    "from aux_evaluate import prepare_mono_for_forward\n",
    "from model import LSTM_Diarization\n",
    "from config import MODEL_FILE\n",
    "\n",
    "\n",
    "model = torch.load(MODEL_FILE, map_location='cpu')\n",
    "\n",
    "\n",
    "model.window_size=20\n",
    "model.shift=10\n",
    "\n",
    "\n",
    "model.train=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auxiliary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux import get_embeddings\n",
    "# from numpy import linalg as LA\n",
    "# from sklearn.cluster import spectral_clustering\n",
    "# from spectral_cluster import blur, row_wise_threshold, row_wise_normalize, symmetrize, diffuse\n",
    "from spectral_cluster import adjust_labels_to_signal\n",
    "\n",
    "\n",
    "\n",
    "def labels_to_audio_size(labels, mono, speech_mono):\n",
    "    \"\"\"labels expect to be boolean np array\"\"\"\n",
    "    \"\"\"True, False stand for first or 2nd speaker\"\"\"\n",
    "    \n",
    "\n",
    "#     print('speech mono sum', speech_mono.sum())\n",
    "#     print('mono shape', mono.shape)\n",
    "    \n",
    "    \n",
    "    adjusted_size_labels = adjust_labels_to_signal(labels, speech_mono.sum())\n",
    "#     print('adjusted', adjusted_size_labels.shape)\n",
    "    labels_audio_size = np.full((mono.shape[0],), 3)\n",
    "    print('labels', labels_audio_size.shape)\n",
    "    labels_audio_size[np.where(speech_mono)] = adjusted_size_labels\n",
    "    \n",
    "    spk_0 = np.zeros_like(mono, dtype=np.bool)\n",
    "    spk_1 = np.zeros_like(mono, dtype=np.bool)\n",
    "    \n",
    "    spk_0[np.where(labels_audio_size==0)] = True\n",
    "    spk_1[np.where(labels_audio_size==1)] = True\n",
    "    \n",
    "    return spk_0, spk_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model forward on single recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_annots = get_char_labels(pred_labels, audio_split, speech_indexes)\n",
    "from spectral_cluster import adjust_labels_to_signal\n",
    "\n",
    "speech_indexes = vad.detect_speech(signal=mono, sampling_rate=8000, fit_to_audio=True)\n",
    "speech_indexes = np.logical_and(speech_indexes, ~overlapping_speech)\n",
    "filtered_channel = mono[speech_indexes]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feats = prepare_mono_for_forward(filtered_mono_channel=filtered_channel, sampling_rate=8000).to('cpu')\n",
    "\n",
    "prepared_feats = prepare_mono_for_forward(filtered_mono_channel=filtered_channel, sampling_rate=8000)\n",
    "d_vectors = model.forward(prepared_feats)\n",
    "\n",
    "from spectral_cluster import get_affinity_matrix, cluster_affinity\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\n",
    "s = get_affinity_matrix(d_vectors.squeeze(0))\n",
    "pred_labels = cluster_affinity(s.detach().cpu().numpy(),dtype=np.int)\n",
    "\n",
    "print(pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spk_0, spk_1 = labels_to_audio_size(pred_labels, mono, speech_indexes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(6, 1, 1)\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.ylim(-1., 1.)\n",
    "plt.plot(channel_0)\n",
    "plt.title(\"channel_0\", x=0.5, y=0.6)\n",
    "plt.fill_between(range(speech_0.shape[0]), speech_0 * .8,color='red', alpha=0.7)\n",
    "\n",
    "\n",
    "plt.subplot(6, 1, 2)\n",
    "# plt.tight_layout()\n",
    "plt.ylim(-1., 1.)\n",
    "plt.plot(channel_1)\n",
    "plt.title('channel 1', x=0.5, y=0.6)\n",
    "plt.fill_between(range(speech_1.shape[0]), speech_1 * .8,color='green', alpha=0.7)\n",
    "\n",
    "plt.subplot(6, 1, 3)\n",
    "plt.title('spk_A_prediction', x=0.5, y=0.6)\n",
    "plt.fill_between(range(spk_0.shape[0]), spk_0 * .8,color='purple', alpha=0.7)\n",
    "\n",
    "plt.subplot(6, 1, 4)\n",
    "plt.title('spk_B_prediction', x=0.5, y=0.6)\n",
    "plt.fill_between(range(spk_1.shape[0]), spk_1 * .8,color='purple', alpha=0.7)\n",
    "\n",
    "plt.subplot(6, 1, 5)\n",
    "plt.title('speech mono', x=0.5, y=0.6)\n",
    "plt.fill_between(range(speech_mono.shape[0]), speech_mono * .8,color='gray', alpha=0.7)\n",
    "\n",
    "plt.subplot(6, 1, 6)\n",
    "plt.title('mono', x=0.5, y=0.6)\n",
    "plt.fill_between(range(mono.shape[0]), mono * .8,color='gray', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform on all talkbank callhome 8 sec recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "audio_files = os.listdir(CALLHOME_ENG_10_SEC)\n",
    "# print(audio_files)\n",
    "\n",
    "for audio_file in audio_files:\n",
    "    try:\n",
    "        fp = CALLHOME_ENG_10_SEC + audio_file\n",
    "        print(audio_file)\n",
    "        sample_rate, audio = load_audio(str(fp), 8000, mono=False)\n",
    "\n",
    "        plt.figure(figsize=(14, 5))\n",
    "\n",
    "        vad=None                                                                                                                                                    \n",
    "        with open(GMM_FILE, 'rb') as fid:                                                                                                             \n",
    "            vad = pickle5.load(fid)                \n",
    "\n",
    "        channel_0, channel_1 = audio[0], audio[1]\n",
    "        speech_0 = vad.detect_speech(signal=channel_0, sampling_rate=8000, fit_to_audio=True)\n",
    "        speech_1 = vad.detect_speech(signal=channel_1, sampling_rate=8000, fit_to_audio=True)\n",
    "\n",
    "        overlapping_speech = np.logical_and(speech_0, speech_1)\n",
    "        channel_0, channel_1 = channel_0/32768, channel_1/32768\n",
    "\n",
    "        _, mono = load_audio(fp, 8000, mono=True)\n",
    "        speech_mono = vad.detect_speech(signal=mono,sampling_rate=8000, fit_to_audio=True)\n",
    "\n",
    "        draw_speech_and_channel(speech_mono, mono, 'mono', 'purple')\n",
    "\n",
    "        speech_indexes = vad.detect_speech(signal=mono, sampling_rate=8000, fit_to_audio=True)\n",
    "        speech_indexes = np.logical_and(speech_indexes, ~overlapping_speech)\n",
    "        filtered_channel = mono[speech_indexes]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        feats = prepare_mono_for_forward(filtered_mono_channel=filtered_channel, sampling_rate=8000).to('cpu')\n",
    "\n",
    "        # prepared_feats = prepare_mono_for_forward(vad=vad, mono_channel = mono, sampling_rate=8000)\n",
    "        prepared_feats = prepare_mono_for_forward(filtered_mono_channel=filtered_channel, sampling_rate=8000)\n",
    "        d_vectors = model.forward(prepared_feats)\n",
    "\n",
    "        from spectral_cluster import get_affinity_matrix, cluster_affinity\n",
    "        import sys\n",
    "        import numpy\n",
    "        numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\n",
    "        s = get_affinity_matrix(d_vectors.squeeze(0))\n",
    "        pred_labels = cluster_affinity(s.detach().cpu().numpy(),dtype=np.int)\n",
    "        \n",
    "        print(pred_labels)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        spk_0, spk_1 = labels_to_audio_size(pred_labels, mono, speech_indexes)\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        plt.subplot(6, 1, 1)\n",
    "        # plt.tight_layout()\n",
    "\n",
    "\n",
    "        plt.ylim(-1., 1.)\n",
    "        plt.plot(channel_0)\n",
    "        plt.title(\"channel_0\", x=0.5, y=0.6)\n",
    "        plt.fill_between(range(speech_0.shape[0]), speech_0 * .8,color='red', alpha=0.7)\n",
    "\n",
    "\n",
    "        plt.subplot(6, 1, 2)\n",
    "        # plt.tight_layout()\n",
    "        plt.ylim(-1., 1.)\n",
    "        plt.plot(channel_1)\n",
    "        plt.title('channel 1', x=0.5, y=0.6)\n",
    "        plt.fill_between(range(speech_1.shape[0]), speech_1 * .8,color='green', alpha=0.7)\n",
    "\n",
    "        plt.subplot(6, 1, 3)\n",
    "        plt.title('spk_A_prediction', x=0.5, y=0.6)\n",
    "        plt.fill_between(range(spk_0.shape[0]), spk_0 * .8,color='purple', alpha=0.7)\n",
    "\n",
    "        plt.subplot(6, 1, 4)\n",
    "        plt.title('spk_B_prediction', x=0.5, y=0.6)\n",
    "        plt.fill_between(range(spk_1.shape[0]), spk_1 * .8,color='purple', alpha=0.7)\n",
    "\n",
    "        plt.subplot(6, 1, 5)\n",
    "        plt.title('speech mono', x=0.5, y=0.6)\n",
    "        plt.fill_between(range(speech_mono.shape[0]), speech_mono * .8,color='gray', alpha=0.7)\n",
    "\n",
    "        plt.subplot(6, 1, 6)\n",
    "        plt.title('mono', x=0.5, y=0.6)\n",
    "        plt.fill_between(range(mono.shape[0]), mono * .8,color='gray', alpha=0.7)\n",
    "\n",
    "        plt.show()\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
